\documentclass[12pt]{report}
\setcounter{secnumdepth}{3}
\usepackage[T1]{fontenc}  % Font encoding
\usepackage{mathptmx}     % Choose Times font 
\usepackage{microtype}    % Improves line breaks      
\usepackage{blindtext}    % Filler text 
\usepackage{hyperref}     % for urls with /url{}

% For our bibliography
\usepackage[backend=biber, sorting=none, hyperref=true]{biblatex}
\addbibresource{\jobname.bib}

%Margin --- 1 inch on all sides
\usepackage[letterpaper]{geometry} \usepackage{times}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{xparse}
\usepackage[font=small,labelfont=bf]{caption}
\NewDocumentCommand{\codeword}{v}{%
	\texttt{\textcolor{blue}{#1}}%
}
\geometry{top=1.0in, bottom=1.0in, left=1.0in, right=1.0in}
% Listings to show code snippets in the report
\usepackage{listings}
% Use as `\begin{lstlisting}...\end{lstlisting}`
\lstdefinestyle{custom}{ 
    language=Python,
    basicstyle=\small,
    breaklines=true,
    frame=single,
    numbers=none,
    tabsize=2, 
    showspaces=false,
    showstringspaces=false,
    keywordstyle=\bfseries\color{green!40!black},
    commentstyle=\itshape\color{purple!40!black},
    identifierstyle=\color{black},
    stringstyle=\color{orange},
}
\lstset{aboveskip=10pt,belowskip=10pt}
\lstset{style=custom}
% For inline code snippets.  use `\code{var = "Hello!"}`
\definecolor{codegray}{gray}{0.9}
\newcommand{\code}[1]{\colorbox{codegray}{\texttt{#1}}}

\usepackage{indentfirst} % Indent the first paragraph after section headings
\setlength{\parskip}{1em} % Set inter-paragraph spacing
\setlength{\parindent}{2em} % Set paragraph indentation
\usepackage{titlesec}
\usepackage{etoolbox}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title page
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
	
	\titlepage{
		\vspace*{150px} % Push down title
		\centering % Centre title
		{\large \sc A Report on a Kademlia Implementation\\}
		\vspace*{30px} % Push down sub text
		\hrulefill \\
		\vspace*{20px}
		{\Huge \uppercase{Kademlia\\}} % Title
		\vspace*{20px}
		\hrulefill \\
		\vspace*{30px}
		April 5th, 2019\\
		\vspace*{180px}
		
		\raggedright
		\hfill
		\textit{CMPT 434: Computer Networks} \\
		\vspace*{8px}
	    Christopher	Mykota-Reid
		\hfill
		Derek Eager\\
		Rowan MacLachlan\\
	}
	
	\pagebreak % Put title on its own page
	
	\tableofcontents
	
	\pagebreak

    \section{Introduction\label{introduction}}
    DHTs are similar in principle to a regular hash table in that they provide
    the structure for fast retrieval of an object by referencing the location
    of the object's hash value.  However, a distributed hash table is stored
    across nodes in a network, and this introduces a variety of complications
    involving node-lookup (finding the location of the stored data from its
    hash in the network), ensuring data redundancy (what happens if a node goes
    offline?), and data retrieval.  Although this technology is not
    \textit{new} by any standard, its continued use and development in various
    fields makes it a suitable candidate for an implementation project.

    Implementing the Kademlia protocol provided an excellent learning
    opportunity with respect to various course materials:
    \begin{itemize}
        \item practical application-level network-related programming, 
        \item the use of some \textit{*NIX} network tools (\textit{ifconfig,
            ip, netstat}, etc.), 
        \item the application of some course theory (implementing
            application-layer communication across different networks over UDP
            implies some difficulties with port-forwarding at the router ---
            this is what NAT would help with,)
        \item a much deeper understanding of the Kademlia protocol, and
        \item an understanding of some of the issues overlay networks (DHTs in
            particular) must deal with.
    \end{itemize}
   
    \section{Objectives\label{objectives}}
    As referenced above, the problems presented by a DHT implementation are
    myriad and often difficult to solve, but by no means impossible.  By implementing
    an incremental plan to gradually roll out more complex features, taking
    full advantage of existing libraries, and using a high-level and expressive
    programming language, we managed to implement a DHT.  While this DHT does
    not implement every particular directive of the Kademlia white paper
    exactly (nor is every successfully implemented feature necessarily
    bug-free), we managed to create an application that could be easily run and
    could, without issue, support a small network of up to 8 devices.

    On one level, the implementation team is very happy with this result,
    considering the considerable hours invested in research, implementation,
    and testing.  Although we failed to produce a working proto-type with
    enough left-over time to collect, interpret, and display data from
    large-scale tests (a hundred or more nodes), we feel as though the
    preliminary testing and class demo show at least some acceptable level of
    achievement.  While this may be disappointing, it must be noted that many
    excellent Kademlia implementations already exist, most of which feature
    more finely-tuned, bugless, and efficient code.  In this sense, simulation
    data would only have cemented an already clear understanding of our
    implementation's shortcomings.  Implementation decisions were almost always
    made by giving far more weight to deadlines than to writing performant
    code.  After all, premature optimization is the root of all evil.
    
    More importantly, we exceeded our personal academic goals --- with respect
    to interest, learning, and technical achievement, the project was a
    complete success.  
    
    \section{Implementation}
        \subsection{Use\label{use}}
            Using the program is relatively simple.  The project folder, once
            downloaded, either provided by the students or downloaded from the
            GIT repository
            \href{https://github.com/rowan-maclachlan/cmpt-434-proj}{here}, has
            a simple structure.  The source code is in the \code{kademlia}
            folder.  So long as the user has a python3.7 (or greater)
            environment, everything should work.
            \begin{enumerate}
                \item Configure your python environment so that the
                    \code{python3} command points to an installation of
                    python3.7 or greater.
                \item \code{git clone
                    https://github.com/rowan-maclachlan/cmpt-434-proj}
                \item \code{cd cmpt-434-proj}
                \item \code{make init}. This will install the application
                    dependencies.
                \item \code{python3 kad.py}.  This will launch a single
                    Kademlia node.  To see application use, run the program
                    without arguments.
                \item Use command \code{set}, \code{get}, \code{ping}, and
                    \code{inspect} to interact with the DHT.
            \end{enumerate}
            In addition to this relatively simple use, the repository also
            contains a script to launch an arbitrary number of nodes to run in
            a non-interactive way.  More details exist within the script
            itself, but this simulation script is operated as such:
            \code{./simulation.sh 10 127.0.0.1 2000}.  This launches 10 nodes
            that bootstrap off of an already existing node located at
            \code{127.0.0.1:2000}, but these nodes don't accept user input.
            Instead, they can be used to test the correctness and robustness of
            the system.

            Please view the README for further details.
        \subsection{High-Level Overview and Tooling}
            \subsubsection{Networking}
                The Kademlia paper implies that the protocol is implemented
                with datagrams\cite{kademlia}.  Typically, clients operating
                behind a NAT (Network Address Translator) do not need to worry
                about querying servers outside the NAT, as the Network Address
                Translator maintains a table correlating incoming messages to
                hosts on its network.  However, on peer-to-peer systems,
                clients must also act as servers, and accept requests from
                other nodes on the overlay network despite being located behind
                a NAT.  This requires the use of a NAT traversal technique such
                as Hole-Punching or SOCKS (Socket Secure) to overcome the
                difficulties inherent in routing requests from LANs over the
                internet.  This makes application use on LANs behind a router
                address difficult without port-forwarding and configuration
                overhead.  Because of this, our application was tested only
                within a LAN.
            \subsubsection{RPC library}
                Our primary investigation showed other Kademlia
                implementations using the core python library asyncio to manage
                callback procedures for asyncronous code execution. While the
                semantics of asynchronous RPC might make implementation
                \textit{more} difficult than for a simple synchronous I/O
                implementation (the python socket module directly, for
                example), implementing asynchronous UDP RCP with asyncio is
                very simple.  In fact, we used a 3rd party library called
                rpcudp which overlays functionality for remote procedure calls
                onto a datagram communication protocol - all
                asynchronously!\cite{rpcudp}

                As described on the asyncio Read-The-Docs page: "asyncio is
                often a perfect fit for IO-bound and high-level structured
                network code." This is exactly the Kademlia
                use-case.\cite{asyncio}

            \subsubsection{Hashing\label{hashing_design}}
                Kademlia specifies the use of the 160-bit SHA-1 hash for data,
                and 160-bit node IDs and 160
                \code{k-buckets} (\ref{bucket_design}), to accompany that.
                This parameter of ID and hash length, however, is not mandated.
                The python library hashlib provides an array of hashing
                functions, cryptographic and otherwise, including the SHA-1
                hash.  This library can be used behind a small wrapper, and
                hash/ID size can be trimmed to the length specified in the
                simulation parameters.  This value could initially be set quite
                small to increase ease-of-testing and provide more
                comprehensible and tractable output during development stages.
                It can easily be changed later.
\begin{lstlisting}[label=hash_function]
def hash_function(data):
    """
    Hash the data to a byte array of length p.params[B] / 8

    Parameters
    ----------
    data : binary data
    
    Return
    ------
    int : The digest of the input data, trimmed to the ID space.
    """
        
    return int(hashlib.sha1(data).hexdigest(), 16) & get_mask() 
\end{lstlisting}
            
            As shown here, the returned digest is ANDed with a mask --- this
            mask is the length of the system-wide hash/ID bit length value ---
            called `B' in the Kademlia paper.  This mask then truncates the
            SHA-1 hash of the data to a value between 0 and $2^b-1$.  This
            allows the value of `B' to be effortlessly changed for more
            practical use of the network or for larger-scale simulations.

            \subsubsection{Routing Table Design\label{routing_table_design}}
                Study of different Kademlia
                implementations\cite{implementation_01}\cite{implementation_02}\cite{implementation_03}\cite{implementation_04}
                reveal that most implementations implement their routing table
                in a way very similar to that described in the Kademlia white
                paper\cite{kademlia}.  This involves creating nodes with only a
                single \code{k-bucket}\ref{bucket_design}.  As the k-bucket
                reaches capacity, it is then split into two and its contents
                distributed among the two resultant buckets according to their
                proximity to the node that owns the routing table.  In this
                sense, a routing table may not actually have as many
                \code{k-bucket}s
                in memory as there are bits in the keyspace.  Naturally, this
                is far more space efficient than the implementation we opted
                for.

                As the maximum number of \code{k-bucket}s on any particular
                node is simply the length of the ID space, they can all be
                created at once, as in the initialization code for a routing
                table \ref{routing_table}:
\begin{lstlisting}[label=routing_table, caption={RoutingTable()}]
self.buckets = [ KBucket(k) for _ in range(b) ]
\end{lstlisting}
                Instead of attempting to insert nodes into a \code{k-bucket}
                which may or may not be full and which may result in further
                operations on the data structure, nodes can be inserted
                directly into the \code{k-bucket} corresponding to their
                distance from the routing table's owner, as shown in
                \ref{routing_table_add}:
\begin{lstlisting}[label=routing_table_add, caption=RoutingTable.add()]
return self.get_bucket(contact.getId()).add(contact)
\end{lstlisting}
                where the method \code{RoutingTable.get\_bucket(id)} calculates
                the correct bucket index by referencing the most significant
                bit of the distance between the ID of the routing table's owner
                and the ID of the contact we are trying to add to the routing
                table, like so:
\begin{lstlisting}[label=routing_table_get_bucket, caption={RoutingTable.get\_bucket()}]
distance = self.id ^ id
index = 0
while distance > 1:
    # Count the index of the largest bit in the distance
    # The distance will be zero after 'bit' many shifts.
    distance = distance >> 1
    index += 1
return self.buckets[index]
\end{lstlisting}
                Naturally, \code{k-bucket}s can be retrieved in a similar
                manner.
                
                Although this approach is less space efficent than creating
                \code{k-bucket}s only as needed, the overhead is fixed and
                relatively small.  Ultimately, this decision was made because
                it was believed to be simpler.

            \subsubsection{Bucket Design\label{bucket_design}}
                The \code{k-bucket} is a container
                that holds contact information for other nodes on the Kademlia
                network.  Each \code{k-bucket} is created with a maximum
                capacity of \code{k}, a system-wide parameter that controls for
                such things as \code{k-bucket} size.  In our implementation, it
                is simply a light wrapper for a list into which contacts are
                stored.
                
                As described by the white paper, \code{k-bucket}s, once full,
                need to make evaluations about whether or not new contacts are
                inserted into the \code{k-bucket} after it has reached capacity.
                When a new node is discovered but its \code{k-bucket} is full,
                it is only added if the node at the head of the list is
                unresponsive.  Otherwise, the new node is simply discarded, and
                the responsive node is moved from the head to the tail of the
                list.

                There are a variety of reasons for this behaviour.  First, the
                Kademlia authors illustrate that nodes which have existed on
                the network for a long time are statistically more likely to
                continue being active on the network than new nodes.
                Therefore, the integrity of the network as a whole is improved
                by prioritizing these old nodes.  Secondly, this behaviour
                makes the network automatically resistant to Denial-of-Service
                attacks: the network cannot be brought down by flooding it with
                new nodes, because the old network will not replace legitimate
                nodes that already exist in its routing table unless they
                become unresponsive.

                Unfortunately, our implementation does not track node
                responsiveness, as the minimum-viable-product was simpler if
                new nodes are simply added (because they are known to be
                active) and old nodes are kicked out of the routing table.

\begin{lstlisting}[caption=KBucket.add(), label=kbucket_add]
if contact in self.contacts:
    # If this contact already appears in the list, move it to the back
    # of the list
    self.contacts.remove(contact)
    self.contacts.append(contact)
elif not self.full():
    # If the contact doesn't appear in the list, append it.
    self.contacts.append(contact)
else:
    # If the list is full, remove the oldest contact before appending
    # the new one.
    self.contacts.popleft()
    self.contacts.append(contact)
return True
\end{lstlisting}
            Because most operations on the \code{k-bucket} involve adding and
            removing nodes from the beginning and end of the list, a
            special-use data structure was employed to optimize the efficiency
            of these operations.  Python provides a structure called a `deque'
            (pronounced `deck') that implements constant-time additions and
            removals from the head and tail of the list.

        \subsection{Kademlia Remote Procedure Calls}
            \subsubsection{Store \label{store}}
               The store operation is used to save a value onto the Kademlia
               network.  Our application accepts a \code{String}-type key and a
               \code{String}-type value, as such: \code{set my-key my-value}.
               The key digest is then taken, which maps the key to a value in
               the node-space range (between 0 and $2^b-1$).  The node then
               attempts to find the closest node on the network to the key
               digest via the \code{find\_nodes} \ref{find_nodes} algorithm.  It
               then issues a store request to $k$ of the nodes closest to the
               value which it found in its search.  A store request message
               consists of the sender's ID, the digest, and the value.  A node
               that receives a store request assumes it is responsible for the
               key and stores the value without exception.  Because the sender
               ID is sent along with the message, the receiving node can take
               steps to properly handle the new contact via the
               \code{handle\_node} procedure (see \ref{handle_node}).
            \subsubsection{Find Value\label{find_value}}
                This operation is used to find a value on the network.
                Implementation depends on the iterative Node Find algorithm
                defined below \ref{find_nodes}.  When this command is received
                by a node, it returns to the sender either the value requested
                (if the receiver has the value in its data store) or a list of
                the $k$ closest nodes in its routing table to the value being
                sought.
            \subsubsection{Find Node}
                This operation is very similar to the Find Value
                \ref{find_value} operation.  The difference is that the node
                issuing the request does not need to anticipate receiving a
                value \textit{or} a list of contact --- only contacts are
                returned by the receiving node.
            \subsubsection{Ping}
                This operation is used to check for liveness on the network.
                Specifically, it is used by nodes to determine whether a new
                contact should replace an old contact in its routing table.  If
                the old contact is unresponsive (it does not respond to the
                \code{ping} operation), it will be removed from the routing
                table in favour of the new contact, which is known to be live.
                This is described in more detail in the implementation section
                \ref{bucket_design} on \code{KBucket} design.
            \subsubsection{Finding Nodes on the Network\label{find_nodes}}
            Searching on the Kademlia Network is done using an iterative,
            parallel algorithm. There are 2 parameters that control the search:
            $\alpha$ and $k$ (the same $k$ referred to in k-buckets). $\alpha$
            controls the number of parallel queries active at once and $k$
            controls the number of nodes returned by each query of a node as
            well as the maximum number of nodes queried before the search
            terminates. The nodes queried are taken from a list of nodes,
            ordered by their distance to the target, called the shortlist. The
            algorithm works by selecting the $\alpha$ closest nodes to the
            target from the shortlist and querying them for nodes. These
            queried nodes respond with their $k$ closest contacts to the target
            (retrieved from their routing table), which could contain the
            target itself. The contacts are then put into the shortlist to be
            selected for future queries. If no node is found closer to the
            closest node found so far then the $k$ closest nodes not yet queried
            are queried, resulting in the termination of the search after the
            responses are processed.
		
		This process is repeated until one of three conditions is met: $k$ nodes
		have been queried, the shortlist is empty, or we have found the target 
		we were searching for. Upon completion of the search, the node that 
		called the search is returned either:
        \begin{itemize}
            \item the $k$ closest nodes to the target found, or,
            \item a value, if the search was a value search.
        \end{itemize}
		
        The other search used in Kademlia is a Value Search. This search
        functions almost identically to the Node Search --- differing only if
        the target is found. 
        \begin{itemize}
            \item If the target is found in a Node Search, the $k$ closest
                contacts to the target are returned. 
            \item In a Value Search, only the value associated with the target
                is returned. A store \ref{store} is then issued to the closest
                node found in the search that did not have the target data. 
        \end{itemize}
        If a Value Search fails (probably because the Kademlia network does not
        contain a mapping for the key), then no store is performed and the $k$
        closest nodes to the target are returned, just as in the Node Search.

    \section{Measurements and Criteria}
    As referenced previously (\ref{objectives}), we failed to collect any
    meaningful simulation data, and so network performance was not measured.
    The only quantitative measurement of project success we have is the number
    of active nodes we could launch on the same network.  Preliminary testing
    revealed we could successfully launch as many as 10 nodes on the same
    network, and observe correct performance of node discovery, set, and get
    operations.  This was achieved through a small script to launch multiple
    nodes at once, called \code{simulation.sh}.

    As described in \ref{use}, using this script allows the user to make an
    evaluation of the system behaviour, robustness, and correctness.  Although
    no meaningful performance data are collected from this simulation, the
    infrastructure is in place.  

    \section{Technical Difficulties}
    Here, we list some of the technical considerations and their resolutions:
    \begin{itemize}
        \item How many nodes store the data of a single object?  What degree of
            redundancy can be achieved and what overhead does this add?
            
            The Kademlia paper invokes this concern, but does not provide any
            hard-numbers.  According to the authors, this number must be high
            enough that even on very difficult networks, no more than this
            number of nodes in the same key-space go offline within an hour of
            each other.  The authors provide a value of 20, but we tested with
            a far smaller value so that correct behaviour could be more easily
            observed.

        \item Is an entire object stored on each node, or do we distribute a
            single object in incomplete parts through the network?

            Our implementation accepted only \code{String} types as keys and
            values.  Regardless of the message type (node list, node id, key,
            or value) the entire message is always transmitted in a single UDP
            packet.  This limits message size to a theoretical value of 65,507
            bytes minus the overhead incurred by the rpcudp library, but our
            implementation did not account for messages sizes in any way.
            Presumably, the user could crash their node by trying to store a
            massive value.

        \item How large is the address space: how large are the hash keys?  128
            bits?  160 bits?  We may not need to have a large address space for
            our implementation, but just what are the consequences of this
            choice?

            As described in \ref{hashing_design}, key-space size was easily adjustable
            because of system design.  However, the decision was made during
            implementation to keep the value small.  This made it easy to
            manually verify correct network behaviour by calculating distances
            by hand, as well as put more stress onto some system internals such
            as \ref{bucket_design} and \ref{routing_table_design} which behave
            differently depending on the density of nodes around the keyspace.

        \item Do we hash the object's identifier or the object data?  If we hash
            the identifier, how do we enforce unique identifiers?

            Unique identifiers (keys) are not enforced.  If some aspect of
            uniqueness was taken from the node attempting the store, their ID
            or IP/port could be used to create a unique hash. However, This would
            make it difficult for other nodes to request that data (as the hash
            was created with some other node's identifying information.)  A
            better solution would be to request the data mapped to by the
            hash-key before attempting the store, and only allow the store if
            the data was not found.  While this solution is not technically
            difficult, it was deemed unimportant in relation to other features.

        \item Any hash function will have collisions.  How do we manage
            collisions?  Do we chain hash contents at the site of storage or do
            we dissallow colliding files?

            Support for chaining hash-collisions was not implemented.  Data
            storage was managed simply by Python's built-in dictionary type.
            If there was a collision (highly possible on a test-network with a
            smaller than usual key-space) the previous value would be
            overwritten at the node performing the store.  This could be
            avoided by the use of a more powerful container type, but it would
            introduce other questions and corresponding technical issues.

            When requesting a value from a key which maps to a node containing
            chained occurences of that key, which value should be returned?
            There's no way for the node recieving the request to know which
            value the node making the request is after, so should it return
            both values?  This would throw a wrench in the current
            implementation of the \code{get} command, which does not expect
            more than a single value response.

        \item How does the \textit{CAP} theorem (or \textit{Brewer} Theorem)
            inform our implementation? What does it mean for it to be impossible
            for a distributed system to provide all of Consistency,
            Availability, and Partition Tolerance?

            As described by the theorem, providing both consistency and
            availability on well-managed internal networks (our test-case) is
            generally not an issue.  Even so, the Kademlia protocol does an
            excellent job at specifying the responsibilities of nodes to keep
            their routing tables up-to-date and ensure that the key-values they
            are responsible for exist with a high degree of redundancy.  In the
            event of highly-congested networks or networks with partitions, the
            Kademlia protocol (with some small changes to our existing
            implementation) will naturally route around high-latency links.
            This is achieved by the continuous re-ordering of active and
            responsive nodes in the routing table, and the option of storing
            contacts within k-buckets ordered on round-trip-time, which can be
            easily measured.

        \item How did we manage the issues presented by key-space remapping?  
            
            Is it possible for us to avoid the issue of key-space re-mapping
            (changing the node location of data) when adding or removing nodes
            from the network?\\

            Thankfully, this is handled very nicely by the Kademlia protocol.
            The first issue it resolves is re-mapping existing data to a new
            node, and the second issue is caused by nodes leaving the network.
            Image a node $n_1$ joining the Kademlia network.  It does this by
            pinging another node on the network that it knows about, $n_2$.
            When $n_2$ hears about $n_1$ (and $n_1$'s ID), $n_2$ issues
            \code{store} requests to $n_1$ for every piece of data $n_2$ has in
            its routing table for which the following conditions are met:
            \begin{itemize}
                \item $n_1$'s ID is closer to the data (by XOR) than the
                    furthest among our k-many closest already known contacts to
                    the data.
                \item We are closest to the data of any of our already known
                    contacts.
            \end{itemize}
            This ensures that new nodes are holding data they are responsible
            for, and it also minizes the number of extraneous stores being sent
            over the network.  This is perhaps difficult to understand but
            section 2.5 of the Kademlia paper describes it well.  Or, consider
            the code below:
\begin{lstlisting}[caption={Protocol.handle\_node()}, label=handle_node]
def handle_node(self, contact):
...
   # See Kademlia paper section 2.5 on how to incorporate new nodes.
   # We may have to store all values we have which are closer to the
   # new node than they are to us.
   for key, value in self.data.items():
       # find neighbours close to the key value
       nearest_contacts = self.table.find_nearest_neighbours(key)
       # If there are fewer than k neighbours, store the key-value to the
       # new node
       if len(nearest_contacts) < self.table.k:
           log.debug(f"Few contacts, storing data to new contact...")
           # schedule the task in the event loop, continue to next data
           asyncio.create_task(self.try_store_value(contact, key, value))
           continue
       # If there are k neighbours, only store the key-value if the new
       # node is closer to the key the our neighbour furthest from the
       # key, and if we are closer to the new node than any of our
       # neighbours.
       nearest_contact = nearest_contacts[0]
       # are we nearer to this key than nearest_contact is?
       were_nearest = \
               self.this_node.distance(contact.getId()) < \
               nearest_contact.distance(contact.getId())
       furthest_contact = nearest_contacts[-1]
       # Is contact closer to the key than the furthest contact?
       close_enough_to_store = \
               contact.distance(key) < furthest_contact.distance(key) 
       if close_enough_to_store and were_nearest:
           log.debug(f"Storing {data} to new contact...")
           asyncio.create_task(self.try_store_value(contact, key, value))

   self.table.add(contact)

   return True
\end{lstlisting}
    
    The second issue of nodes leaving the network is handled by there
    being a degree of redundancy for data.  Data are replicated by a factor of
    $k$ on the network, and so a single node leaving has no practical effect.

    \end{itemize}

    Please see the bibliography for additional reading.
	
    \pagebreak

\nocite{*}
\printbibliography

\end{document}
